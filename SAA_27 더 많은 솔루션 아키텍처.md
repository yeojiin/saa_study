### AWS의 이벤트 처리
- SQS와 Lambda를 사용하는 방법
	- 이벤트가 SQS 대기열에 삽입되고, Lambda 서비스가 SQS 대기열을 폴링
	- 문제가 발생하면 해당 메시지를 다시 SQS 대기열에 입력하고, 폴링을 재시도
	- 이 작업은 무한히 반복되는데 이러던 중 한 메시지에 중대한 문제가 발생하면 다섯 번의 재시도 후에는 `데드 레터 대기열(DLQ)`로 보내도록 SQS를 설정할 수 있음
- SQS FIFO와 Lambda를 사용하는 방법
	- FIFO란 선입선출(first in, first out)이라는 의미로 도착한 순서에 따라 메시지를 처리한다는 뜻
	- Lambda 함수가 대기열 메시지를 순서대로 처리를 시도하는데 순서대로 처리하기 때문에 한 메시지를 처리하지 못 하면 차단이 발생하여 처리가 끝나지 않고 결국 전체 대기열 처리가 차단
	- 이 경우에도 데드 레터 대기열을 구성해서 SQS 대기열에서 해당 메시지를 빼내고 함수가 계속 동작하도록 할 수 있음
- SNS와 Lambda를 활용
	- 메시지가 통과하고, 메시지는 비동기식으로 Lambda에 전송
	- 이때 Lambda 함수는 처리하지 못하는 메시지가 발생하더라도 내부적으로만 재시도를 함
	- 재시도는 총 세 번까지 하며 메시지가 제대로 처리되지 않으면 해당 메시지를 제거하거나 DLQ로 보내도록 구성할 수 있지만 Lambda 서비스 수준에서 해당 메시지를 SQS 대기열로 보내서 나중에 다시 처리할 수도 있음
	- SQS를 사용할 때는 DLQ를 SQS 측에 설정하거나 Lambda 측에 설정할 수도 있음
- 팬아웃 패턴(fan out pattern)
	- 팬아웃 패턴은 다중 SQS 대기열에 데이터를 전송하는 방식
	- 첫 번째 패턴에서는 애플리케이션과 AWS SDK가 설치되어 있고, 세 개의 SQS 대기열에 메시지를 전송하고자 함
	- 이럴 경우에는 간단히 애플리케이션을 구성할 때 먼저 메시지를 첫 번째 대기열에 보내고, 동일한 메시지를 두 번째 대기열과 세 번째 대기열에도 보냄
	- 이 방식은 작동은 하지만 안정성이 높지는 않음
	- 두 번째 대기열에 메시지를 전송한 다음 애플리케이션이 오류로 종료되면 세 번째 대기열은 메시지를 전달받지 못하므로 결국에 각 대기열의 콘텐츠가 달라짐
	- 이 방법은 안정성도 낮고 구조도 깔끔하지 않음
- 그 대신 팬아웃 패턴을 사용하여 SQS 대기열과 애플리케이션 사이에 SNS 주제를 두면 어떻게 될까?
	- SQS 대기열은 SNS 주제의 구독자가 되고 SNS 주제에 메시지를 전송할 때마다 메시지가 모든 SQS 대기열에 전달되므로 안정성이 높아짐
	- 애플리케이션의 입장에서 보면 SNS 주제에 PUT 요청을 전송하면 자동으로 SNS 서비스가 해당 메시지를 SQS 대기열에 팬아웃하는 것
	- 잘 작동하는 방법이고 AWS에서 자주 보이는 설계 패턴
- S3 Event Notifications
	- Amazon S3 버킷이 특정 이벤트에만 반응하도록 설정할 수 있어 객체 생성, 삭제 복원, 복제본 생성 시 알림을 보내도록 할 수 있고, 객체 이름별로 필터링도 가능
	- 사용 사례로는 Amazon S3에 업로드된 이미지의 섬네일을 생성하는 경우가 있음
	- Amazon S3 이벤트를 SNS SQS 또는 Lambda 함수로 보내는데 이때 S3 이벤트는 원하는 만큼 생성할 수 있음
	- 이벤트 알림은 보통 수초 내로 전송되고 ,가끔 몇 분 이상이 소요될 수도 있음
	- Amazon EventBridge를 사용하는 방법 
		- Amazon S3 버킷에서 일어난 모든 이벤트 를 Amazon EventBridge로 전송하는 방식인데 규칙을 설정하여 18개 이상의 AWS 서비스로 전달할 수 있음
		- 왜 EventBridge를 사용할까?
			- JSON 규칙에 고급 필터링 옵션을 사용하여 메타데이터, 객체 크기, 이름 등으로 필터링할 수 있기 때문
			- 또 여러 대상에 이벤트를 한 번에 보낼 수 있고 예를 들어 단계 함수, Kinesis Data Stream, Firehose 등이 대상이 될 수 있음
			- EventBridge 기능 중 아카이빙, 이벤트 재생, 그리고 안전한 이벤트 전송 등도 있음
		- Amazon EventBridge에서 모든 API 호출을 인터셉트하려면 CloudTrail를 통합하여 사용하면 됨
			- 사용자가 DynamoDB의 테이블을 삭제하고자 DeleteTable API을 호출하는 이벤트가 발생했다면 이 API 호출은 CloudTrail에 로그되며 그 외 모든 호출이 CloudTrail에 로그 됨
			- 이 로그는 Amazon EventBridge의 이벤트를 트리거하므로 이를 이용해 경보를 생성하여 Amazon SNS에 전송할 수 있음
- API Gateway 등을 이용하는 AWS의 외부 이벤트가 있음
	- API Gateway에 클라이언트가 요청을 보내면 API Gateway가 Kinesis Data Stream에 메시지를 전송
	- 해당 레코드는 Kinesis Data Firehose로 이동하고 최종적으로는 Amazon S3에 저장
### AWS의 캐싱 전략
- API Gateway 앞에 CloudFront가 있고, Application Logic 앞에 있는 건 EC2나 Lambda가 될 수 있으며 애플리케이션은 데이터베이스에서 데이터를 저장하고 쓰며 Redis, Memcached 혹은 DAX 등의 내부 캐시를 쓸 수도 있음
	- 동적 콘텐츠와 `동적 라우팅`이 실행됨
- `정적 콘텐츠 라우팅도` 할 수 있는데 CloudFront로 가는 클라이언트나 CloudFront가 S3에서 데이터를 소싱하는 경우
- CloudFront를 살펴보면 두 개가 있는데 <u>엣지에서 캐싱을 함</u>
	- 즉, 사용자와 최대한 가까이에서 캐싱을 한다는 뜻
	- CloudFront에서 캐싱을 허용하면 캐시를 히트하는 모든 사용자들이 즉시 응답을 빠르게 얻게 된다는 것
	- 엣지에 있기 때문에 백엔드에서 변화가 일어났을 수 있음
	- 그리고 몇몇은 오래됐을 수도 있음
	- 캐시가 최신인지 확인하려면 `TTL`을 사용할 수 있음
- API Gateway
	- API Gateway도 캐싱이 가능해서 CloudFront와 함께 사용할 필요가 없음
	- API Gateway는 리전 서비스라서 API Gateway에서 캐시를 사용할 경우 캐시도 리전에 묶이게  됨
	- 클라이언트와 API Gateway 사이에 네트워크 라인이 형성되어 캐시가 여기서 히트
- App logic이 있는지만 보통 캐싱을 하지 않는데 DynamoDB가 있을 경우 Redis, Memcached DAX 등을 사용할 수 있는 캐시를 할 때 사용
	- 데이터베이스를 반복적으로 히트하길 원치 않는 것
	- 데이터베이스는 캐싱을 못 하니까
	- 자주 발생하는 쿼리나 복잡한 쿼리가 공유 캐시에 결과가 저장되어 앱 논리에 의해 쉽게 액세스할 수 있게 하는 것
	- 데이터베이스에 가해지는 압력을 줄이고, 읽기 용량은 늘리는 것
- <u>Amazon S3의 데이터베이스에는 캐싱 기능이 없음</u>
- 캐싱의 경로를 따라가보면 뒤쪽으로 갈수록 비용과 지연 시간이 늘어남
### AWS에서 IP 주소 차단
- IP 주소를 클라이언트로부터 막고 싶어 하는데 말썽이 일어나기도 하고, 애플리케이션에 액세스를 시도하기 때문에 방어선에 대해 알아야 함
- EC2 인스턴스를 사용한 간단한 솔루션 아키텍처
	- VPC의 보안 그룹에서 인스턴스는 공용 IP를 써서 공용 액세스가 가능하고, 클라이언트들도 EC2 인스턴스에 이렇게 접근
	- 클라이언트 차단을 원한다면 먼저 VPC 레벨에서 VPC의 NACL방어가 첫 번째 방어선
	- 이 NACL에서는 차단 규칙을 만들 수 있는데 클라이언트 IP에 대해 아주 간단하고 빠르며 저렴하게 만들 수 있고 클라이언트를 내보내게 됨
- 그리고 <u>EC2 인스턴스의 보안 그룹에는 차단 규칙을 만들 수 없고 허용 규칙만 가능</u>
	- EC2 인스턴스에 액세스 가능한 권한이 부여된 클라이언트들의 서브셋만 알면 IP의 서브셋을 보안 그룹에서 지정해 EC2 인스턴스로 허용하면 됨
- 애플리케이션이 글로벌이라면 애플리케이션에 액세스하는 IP 주소를 전부 알 수도 없고 보안 그룹도 도움이 되지 않음
	- 선택적으로 방화벽 소프트웨어를 EC2 인스턴스에서 실행해 클라이언트의 요청을 거절할 수 있음
	- 만약 요청이 벌써 EC2 인스턴스에 도착했다면 처리가 되고 요청을 처리하는 데에 CPU 비용이 발생
- 애플리케이션 로드 밸런서
	- ALB는 VPC 내에서 정의되고, EC2 인스턴스가 있는데 ALB 보안 그룹이 있고, EC2 보안 그룹이 있음
	- 로드 밸런서는 클라이언트와 EC2 사이에 위치하고, 연결 종료라는 걸 실행
	- 클라이언트는 ALB에 연결하고 , ALB에서 EC2 인스턴스로 연결을 시작하거나 종료
	- EC2 보안 그룹은 ALB의 보안 그룹을 허용하도록 구성돼야 함
	- EC2 인스턴스가 사설 IP와 클라이언트 측이 아닌 ALB로부터 오는 트래픽을 통해 사설 서브넷에 배포될 수 있기 때문
	- 보안 그룹의 관점에선 ALB 보안 그룹만 허용해서 안전해지는 것
- 보안 그룹의 ALB
	- ALB의 보안 그룹에서는 클라이언트를 허용해야 하며 아는 IP의 범위가 있어서 보안 그룹을 구성
	- 글로벌 애플리케이션일 경우 모두를 허용해야 하고, 방어선은 NACL 레벨에 있게 됨
	- 트래픽은 네트워크 로드 밸런서를 거쳐가고, 그 과정에서 네트워크 로드 밸런서를 위한 보안 그룹은 없기 때문에 트래픽이 지나가게 됨
	- 그러니 IP를 새로 만드는 클라이언트는 EC2 인스턴스가 사설 IP를 사용하는 사설 서브넷에 있어도 EC2 인스턴스로 갈 수 있음
	- 모든 클라이언트의 소스 IP를 안다면 EC2의 보안 그룹에서 정의하면 되지만 클라이언트로부터 IP 주소를 하나만 차단하려면 방어를 위해 NACL을 사용해야 함
	- <u>네트워크 로드 밸런서는 보안 그룹이 없고, 모든 트래픽이 지나가기 때문에 EC2 인스턴스는 엣지에서 클라이언트의 공용 IP를 감지</u>
- ALB는 WAF를 설치하거나 웹 애플리케이션 방화벽을 설치해 IP를 차단할 수 있음
	- WAF는 비쌀 수 있는데 추가 서비스이며 방화벽 서비스이기 때문
	- WAF는 IP 주소에 대한 복잡한필터링이 가능하며 규칙을 만들어서 클라이언트로부터 동시에 많은 요청을 받지 않도록 할 수 있고, 따라서 ALB나 보안이 더 강해짐
	- WAF는 클라이언트와 ALB 사이에 있는 서비스가 아니라 <u>ALB에 설치한 서비스</u>로 규칙을 많이 정의할 수 있음
	- ALB 앞에 CloudFront를 사용하면 CloudFront는 VPC 밖에 위치
	- ALB는 모든 엣지 로케이션으로부터 오는 CloudFront의 공용 IP를 허용해야 하고 온라인에 목록이 있음
	- ALB로부터 오는 것은 클라이언트 IP를 알 수 없고, CloudFront 공용 IP만 알 수 있음 
	- NACL은 VPC의 경계선에 있는데 클라이언트 IP를 막는 데 도움이 되지는 않음 -> CloudFront IP만 받기 때문에
- CloudFront로부터의 클라이언트를 막으려면 두 가지 방법이 있음
- 국가에 의해 공격당했다면 CloudFront의 **지리적 제한 기능을** 이용해서 CloudFront에서 클라이언트의 국가를 차단하는 방법이 있음
- 특정 IP 하나만 막고 싶다면 **WAF나 웹 애플리케이션 방화벽을** 사용해서 IP 주소를 필터링하면 됨
- IP 주소를 차단하기 위한 방어선이 달라짐
### AWS의 고성능 컴퓨팅(HPC)
- 고성능 컴퓨팅 혹은 HPC
- 많은 리소스를 즉각적으로 생성할 수 있어 클라우드는 고성능 컴퓨팅을 실행하기에 최적
- 그리고 리소스를 추가해서 결과 추출 시간을 단축할 수 있고, 사용량만큼만 비용을 지불하면 됨
- 작업이 끝나고 나면 전체 인프라를 제거해서 요금이 청구되지 않게 할 수 있음
- 필요에 따라 컴퓨팅을 수행하는 수많은 인스턴스를 가질 수 있고, 작업이 완료되면 사용량만큼만 비용을 지불한다는 것
- 언제 이런 HPC가 필요할까?
	- 유전체학이나 컴퓨터화학 금융 위험 모델링, 기상 예측, 머신 러닝, 딥 러닝 자율 주행 등에 필요
- HPC의 작업을 돕는 AWS 서비스는 무엇이 있을까?
- 첫 번째는 데이터 관리 방식과 AWS로 데이터를 전송하는 방법에 관한 것
- AWS Direct Connect는 초당 GB의 속도로 프라이빗 보안 네트워크를 통해 클라우드로 데이터를 전송
- Snowball과 Snowmobile도 있음
	- 물리적 라우팅을 통해 클라우드로 PB 단위 데이터를 옮길 때 사용하고, 주로 대용량 전송에 아주 적합
- DataSync의 경우 DataSync 에이전트를 설치해 대용량의 데이터를 전송
	- 온프레미스, NFS, SMB 시스템에서 S3, EFS나 Windows용 FSx로 전송
- HPC의 컴퓨팅과 네트워킹은 어떨까?
- EC2 인스턴스가 있음
	- 실행하려는 작업에 따라 CPU나 GPU에 최적화된 인스턴스가 있음
- 스팟 인스턴스나 스팟 플릿(Fleet)을 사용해 비용을 크게 절약하고 수행하는 계산을 기반으로 플릿을 오토 스케일링할 수 있음
- EC2 인스턴스가 서로 통신해야 하거나 분배된 형태로 동작할 경우 클러스터 유형의 EC2 배치 그룹을 사용하면 최고의 네트워크 성능을 발휘
- 짧은 지연 시간의 10Gbps 네트워크가 될 것
- 클러스터 배치 그룹의 경우 랙(Rack)이 전부 같음
	- 모두 같은 AZ에 있는 것
- EC2 인스턴스의 성능을 더 향상하는 방법은 무엇일까?
- EC2 Enhanced Networking입니다 또한 SR-IOV라고도 하는데 더 넓은 대역폭이 제공되고 더 높은 PPS 즉 초당 패킷이 높아지며 지연 시간이 짧아짐
- EC2 Enhanced Networking은 어떻게 구현할까?
	- **Elastic Network Adapter(ENA)**
	- 네트워크 속도를 100Gbps까지 올려줌
- ENA는 대역폭과 초당 패킷을 증가시키며 지연 시간을 줄여줌
- 두 번째는 Intel의 82599VF라는 걸 사용해 최대 10Gbps까지 빨라집니다 오래된 ENA
- ENA와 Intel 모두 인스턴스에서 EC2 Enhanced Networking을 이용할 수 있게 함
- Elastic Fabric Adapter(EFA)를 사용해도 됨
	- 분산 계산과 같은 노드 간 소통이나 밀집된 워크 로드 처리에 좋음, 클러스터 배치 그룹 등
	- HPC, 고성능 컴퓨팅을 위해 개선된 ENA인데 <u>Linux에서만 사용 가능</u>
	- Linux OS를 우회해 안정적이고 지연시간이 더 짧은 송신을 보장
	- Linux 인스턴스가 있고 많은 워크로드를 처리할 경우 `EFA` 사용
- 데이터는 어떻게 저장할까?
- 인스턴스가 연결된 스토리지를 사용하는 방법이 있음
	- **EBS**의 경우 io2 Block Express로 256,000IOPS까지 확장 가능
	- **인스턴스 스토어**의 경우 수백만의 IOPS로 확장해요 EC2와 연결되어 하드웨어에 있고, 지연 시간이 짧지만 인스턴스가 망가지면 손상될 수 있음
	- Amazon S3 등의 **네트워크 스토리지**를 써도 됨
		- 대용량 블롭 데이터를 저장할 때 사용
		- 파일 시스템 말고 큰 객체 저장을 위한 것
	- **EFS**를 사용하면 IOPS가 파일 시스템의 전체 크기에 따라 확장
		- 프로비저닝된 IOPS 모드를 써서 EFS에서 높은 IOPS를 얻기도 함
- HPC 전용 파일 시스템이 있었는데 `FSx for Lustre`라고 합니다 Lustre는 <u>Linux와 Cluster용이고</u>, HPC에 최적화되어 수백만의 IOPS를 제공하며 백엔드에서 S3로 제공
- 자동화 및 오케스트레이션은 어떨까?
	- 먼저 `AWS Batch`를 사용하면 다중 노드 병렬 작업을 수행할 수 있고, 여러 EC2 인스턴스에 걸쳐 작업할 수 있음
	- AWS Batch를 사용하면 작업 예약과 AWS Batch 서비스로 관리되어 EC2 인스턴스 실행이 쉬워짐
	- 때문에 AWS Batch는 HPC에서 많이 선택하는 방법
- **AWS ParallelCluster**는 오픈 소스 클러스터 관리 도구로 HPC를 AWS에 배포
	- 텍스트 파일로 구성해서 AWS로 배포하는 것
	- VPC와 서브넷 및 클러스터 타입과 인스턴스 타입 생성을 자동화
	- AWS ParallelCluster는 EFA와 함께 사용
		- 클러스터 상에서 EFA를 활성화하는 매개변수가 텍스트 파일에 있기 때문에 네트워크 성능이 향상되고, HPC 클러스터를 구현할 수 있음
- HPC는 단일 서비스가 아니라 여러 옵션과 서비스의 결합
### EC2 인스턴스 고가용성
- 솔루션 아키텍처를 통해 EC2 인스턴스의 가용성을 높일 수 있음
- EC2 인스턴스는 기본적으로 하나의 가용 영역에서 실행되는 데가용성이 높지 않지만 수동으로 가용성을 높일 수 있음
- 웹 서버를 가동하고 있는 공용 EC2 인스턴스가 있고 웹 서버에 액세스 한다고 가정
	- EC2 인스턴스에 탄력적 IP를 연결하고, 사용자가 탄력적 IP를 통해 웹사이트에 곧장 액세스하게 해야함
	- 사용자는 EC2 인스턴스에서 직접 작업할 수 있고, 우리는 웹 서버에서 결과를 받음
	- 하지만 재해를 대비해 **대기 인스턴스**를 만들어 EC2 인스턴스의 가용성을 높이는 방법
	- 대비 EC2 인스턴스에 장애 조치를 취해야 하는데 <u>장애가 생긴 걸 알기 위해선 모니터링이 필수</u>
	- 이벤트에 기반해 CloudWatch Events나 경보를 만듦
	- CloudWatch Events가 있다면 인스턴스가 종료되고 있는지 확인할 수 있을 것
	- 웹 서버가 있다면 CPU가 100%까지 올라갈 경우를 대비해 CPU를 모니터링하는 CloudWatch 경보를 설정해야 함
	- 만약 CPU가 100%에 도달한다면 EC2 인스턴스에 문제가 생긴 것이고 그에 관한 경보를 발동해야 할 것
		- 인스턴스를 모니터링하는 방법은 여러 개가 있고 요구 사항에 따라 달라짐
	- 그 후 경보나 CloudWatch Events에서 람다 함수를 발동할 수 있는데 람다 함수는 원하는 작업을 할 수 있게 할 것
		- 예를 들어 람다 함수는 API를 호출해 인스턴스가 실행되지 않았을 경우 실행할 수 있음 -> 대기 EC2 인스턴스가 없는 경우
		- 그 후 대기 인스턴스에 탄력적 IP를 연결하는 API를 호출할 수도 있음
		- 한 탄력적 IP는 한 인스턴스에만 연결될 수 있기 때문에 대기 인스턴스에 탄력적 IP가 연결되면 이 IP는 다른 인스턴스에서는 분리됨
		- 다른 EC2 인스턴스는 종료되거나 사라질 것이고, 새 대기 EC2 인스턴스에는 장애 조치가 취해지는 것
		- 하지만 사용자는 탄력적 IP를 통해 아키텍처와 소통하기 때문에 무슨 일이 일어나는지는 알 수 없고, 다 백엔드에서 진행됨
- 오토 스케일링 그룹을 이용하는 방법
	- 두 가용 영역에 오토 스케일링 그룹(ASG)이 있다고 가정
	- 사용자가 탄력적 IP를 통해 애플리케이션과 소통하게 만들어 조금 더 단순화하는 것
	- ASG를 어떻게 구성해야 할까?
		- 인스턴스의 최솟값과 최댓값을 1로 설정하고 적정 값도 1로 설정해 두 개의 가용 영역에 지정하는 것
	- EC2 인스턴스를 하나만 가져오는데 그 인스턴스는 첫 번째 가용 영역에 들어가게 됨
	- EC2 인스턴스의 사용자 데이터가 나타나게 되면 이 탄력적 IP 주소를 태그에 기반해 연결
		- 사용자 데이터가 API 호출을 발행하고 탄력적 IP가 공용 EC2 인스턴스에 연결되는 것
	- 사용자가 웹 서버와 소통할 수 있게 됨
	- 해당 인스턴스가 종료될 경우 ASG는 첫 인스턴스를 종료하고 다른 가용 영역에 대체 EC2를 생성
	- 첫 번째 인스턴스가 종료되고 두 번째 인스턴스가 EC2 사용자 데이터 스크립트를 실행하고 탄력적 IP를 연결
	- **CloudWatch 경보나 Events가 필요 없음**
		- 한 인스턴스가 종료되는 것을 ASG가 보자마자 다른 가용 영역에 새 EC2 인스턴스를 생성하기 때문
	- 최소, 최대, 적정 값을 1로 설정한 이유는 ASG 전체에서 동시에 여러 개의 인스턴스를 실행할 수 없기 때문
	- EC2 인스턴스가 이 탄력적 IP 주소를 연결하기 위해 API를 직접적으로 호출할 때 해당 인스턴스가 탄력적 IP 주소를 연결하기 위해 API를 호출할 수 있는 **인스턴스 역할이** 있는지 확인해야 함
	- EC2 사용자 데이터를 이용해 탄력적 IP 주소를 연결하고 API 호출이 성공할 수 있도록 하기위햐서는?
		- 이 패턴은 다른 영역으로도 확장 가능한데 예를 들어, EC2 인스턴스를 상태 유지하게 하고 EBS 볼륨을 줄 수 있음
		- ASG와 두 가용 영역 공용 EC2 인스턴스와 탄력적 IP가 있음
		- EC2 인스턴스에 EBS 볼륨까지 연결
		- EC2 인스턴스가 데이터베이스라고 가정 이 데이터베이스를 고가용성으로 만든다고 가정
		- 모든 데이터가 EBS 볼륨에 있고 EBS 볼륨은 특정 가용 영역에만 고정되어 있음
		- EC2 인스턴스가 종료된다고 가정
		- 인스턴스가 종료될 경우 ASG는 `수명 주기 후크`를 사용
		- 수명 주기 후크 덕에 EBS 볼륨에서 스냅샷을 얻을 수 있는 스크립트를 생성할 수 있음
		- EC2 인스턴스가 종료되자마자 스냅샷이 발동되기 때문에 EBS 볼륨에 문제가 생겼다는 것을 알 수 있음
		- EBS 스냅샷을 올바르게 태그하면 ASG는 대체 EC2 인스턴스를 실행할 것
		- 실행 이벤트에 수명 주기 후크를 생성하도록 ASG를 올바르게 구성함으로써 이 EBS 스냅샷에 기반해 올바른 가용 영역에 EBS 볼륨을 생성할 수 있음
		- 대체 EC2 인스턴스에 이걸 연결하면 EC2 사용자는 이것만 확인하고, 탄력적 IP를 직접적으로 연결하면 됨
		- API 호출이 제대로 됐는지 확인해야 하니 **EC2 인스턴스 역할이** 있어야 함
		- EBS 볼륨이 스냅샷을 만들고 그 스냅샷에서 다른 가용 영역으로 복구되는지 확인하기 위해 EC2 사용자 데이터 및 수명 주기 후크를 이용 -> EBS 볼륨으로 고가용성 EC2 인스턴스를 만드는 법
